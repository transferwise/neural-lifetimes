{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use your own dataset with `neural-lifetimes`\n",
    "\n",
    "Here we will demonstrate how to use your dataset with this library. For this purpose we use a dataset from Kaggle\n",
    "\n",
    "[https://www.kaggle.com/shailaja4247/customer-lifetime-value-prediction/data](https://www.kaggle.com/shailaja4247/customer-lifetime-value-prediction/data)\n",
    "\n",
    "and use components from the neural lifetimes library to predict customer value.\n",
    "\n",
    "Please download the dataset and place it in a `data` subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n",
    "If one of the dependencies is not installed run `!pip install <package_name>` in a Jupyter cell to add them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset\n",
    "\n",
    "We load the datset and set the columns' datatypes correctly. Further, we clean up the dataset as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'InvoiceNo': str,\n",
    "    'StockCode': str,\n",
    "    'Description': str,\n",
    "    'Quantity': int,\n",
    "    'UnitPrice': float,\n",
    "    'CustomerID': str,\n",
    "    'Country': str}\n",
    "data = pd.read_csv('data/customer_segmentation.csv', encoding='cp1252', dtype=dtypes, parse_dates=['InvoiceDate'])\n",
    "\n",
    "# remove rows with no customerID\n",
    "data = data.dropna(subset=['CustomerID'])\n",
    "\n",
    "# filter out customer returns (e.g. transactions with negative quantities)\n",
    "data = data[data.Quantity > 0]\n",
    "# filter out orders only shipping free items\n",
    "data = data[data.UnitPrice > 0]\n",
    "\n",
    "# log transform quantities and prices for better stability in modelling \n",
    "data['LogUnitPrice'] = np.log(data['UnitPrice'])\n",
    "data['LogQuantity'] = np.log(data['Quantity'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us check out the first few rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that multiple rows refer different products and thus multiple rows can refer to one invoice. We will regard each invoice as one transaction rather than indiviual items. This implies that we need to aggregate the data from each row belonging to the same invoice and thus we do some feature engineering. We will do this very rudimentary and just record the sum, mean and standard deviation for the quantities per product, the prices and the number of products per invoice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_per_invoice(df):\n",
    "    pass\n",
    "    invoices = data.InvoiceNo.unique()\n",
    "\n",
    "    data_per_invoice = data[['InvoiceNo', 'InvoiceDate', 'Country', 'CustomerID']].drop_duplicates().set_index('InvoiceNo')\n",
    "\n",
    "    data_per_invoice['MeanLogUnitPrice'] = data.groupby('InvoiceNo')['LogUnitPrice'].mean()\n",
    "    data_per_invoice['StdLogUnitPrice'] = data.groupby('InvoiceNo')['LogUnitPrice'].std().fillna(0)\n",
    "    data_per_invoice['SumLogUnitPrice'] = data.groupby('InvoiceNo')['LogUnitPrice'].sum()\n",
    "    data_per_invoice['MeanLogQuantity'] = data.groupby('InvoiceNo')['LogQuantity'].mean()\n",
    "    data_per_invoice['StdLogQuantity'] = data.groupby('InvoiceNo')['LogQuantity'].std().fillna(0)\n",
    "    data_per_invoice['SumLogQuantity'] = data.groupby('InvoiceNo')['LogQuantity'].sum()\n",
    "    data_per_invoice['NumProducts'] = data.groupby('InvoiceNo')['LogQuantity'].count()\n",
    "\n",
    "    return data_per_invoice\n",
    "\n",
    "data = get_dataset_per_invoice(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the number of transactions per customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_per_customer = data.groupby(['CustomerID']).size()\n",
    "summary_t_per_c = num_transactions_per_customer.describe()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.DataFrame({'Transactions per Customer': summary_t_per_c})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems fine. We have customers with between 1 and 210 transactions. However, most have <= 5 transactions. We visualise this using a boxplot on the log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(np.log(num_transactions_per_customer))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boxplot marks `exp(4.007)` as maximum number of transactions (using 1.5xIQR rule) and any customers with more transactions as outliers.\n",
    "\n",
    "Next, let us look at the invoice dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First transaction:\", data['InvoiceDate'].min(), \"Last transaction:\", data['InvoiceDate'].max())\n",
    "fig = px.histogram(data, x='InvoiceDate')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that it is fairly well spread with a slight increasing tendancy and peaks before Christmas. We may also observe Christmas breaks.\n",
    "\n",
    "Next, let us look at the distribution of countries associated with each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Countries in Dataset : {data['Country'].nunique()}\")\n",
    "px.histogram(data, x='Country', histnorm='probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may observe a distribution with a very long tail with almost 90% of all transactions being done in the UK. We may also observe that `Unspecified` is included as one country.\n",
    "\n",
    "Finally, let us print the mean and standard deviation for all continuous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use neural-lifetimes\n",
    "\n",
    "We will now use the `neural-lifetimes` library to fit a neural network model predicting sequences of transactions from which one can derive all kinds of quantities, such as customer lifetime values.\n",
    "\n",
    "Our model will take our features and convert them from their original format (e.g. strings) to numberic values and finds an embedding space for them. We feed these embeddings into a time sequence model (here `GRU`) to generate multivariate time series predictions for events using an variational encoder-decoder model. For each feature we apply a small decoder head at the end to predict them for the next time series model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some imports and configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import neural_lifetimes\n",
    "\n",
    "from neural_lifetimes.data.datasets import PandasSequenceDataset\n",
    "from neural_lifetimes.data.dataloaders import SequenceLoader\n",
    "from neural_lifetimes.models.nets import CombinedEmbedder\n",
    "from neural_lifetimes.models import TargetCreator\n",
    "from neural_lifetimes.data.datamodules import SequenceDataModule\n",
    "from neural_lifetimes.models.modules import VariationalEventModel\n",
    "\n",
    "LOG_DIR = str(Path.cwd() / \"logs\")\n",
    "print(f\"Logging to: {LOG_DIR}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    device_type = \"gpu\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_type = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Sequence` pattern implemted in the package: There exist multiple classes in the package named `*Sequence*` that work together to process multivariate sequential data. For example, `SequenceLoader` (a pytorch Dataloader) `SequenceDataset` (a pytorch dataset) or `SequenceDataModule` (a pytorch lightning DataModule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, here we will use `neural_lifetimes.data.datasets.PandasSequenceDataset` to create a sequence dataset. It takes a Pandas Dataframe as input and we only need to specify relevant column names. Read #TODO for full documentation. The `continuous_feature_names` and `category_dict` specify the features our model will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the datset\n",
    "dataset = PandasSequenceDataset(df=data, uid_col='CustomerID', time_col='InvoiceDate', asof_time=datetime.datetime(2012, 1, 1), min_items_per_uid=1)\n",
    "\n",
    "# specify which columns to use\n",
    "continuous_feature_names = ['MeanLogUnitPrice', 'StdLogUnitPrice', 'SumLogUnitPrice', 'MeanLogQuantity', 'StdLogQuantity', 'SumLogQuantity', 'NumProducts']\n",
    "categorical_feature_names = ['Country']\n",
    "category_dict = {}\n",
    "for col in categorical_feature_names:\n",
    "    category_dict[col] = data[col].unique()\n",
    "print(category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the\n",
    "- Tokens (we have variable length time sequenes, so we need to communicate with the model when a new series starts and an old one ends).\n",
    "- Logging directory to save model checkpoints and logs\n",
    "- Set device to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN_DISCR = 'StartToken'\n",
    "START_TOKEN_CONT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to decide on how to embed our data. We will use the following:\n",
    "\n",
    "- `Country` will be parsed through the discrete embedder\n",
    "- The features derived from `UnitPrice` and `Quantity`, as well as `NumProduct` will get a continuous embedding\n",
    "\n",
    "We are ignoring the remaining columns for this demonstration, but it would be possible to add more complicated embeddings and more columns. For example, we might add a `BERT` embedding for the description of product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our lists of features, we will create two key parts of the training pipeline:\n",
    "\n",
    "1. The `CombinedEmbedder` processes all features individually and finds neural embeddings before parsing them on to the time series modelling.\n",
    "2. The `TargetCreator` implements the decoding for each feature and sets up the appropriate loss for them.\n",
    "\n",
    "#TODO Link to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = CombinedEmbedder(\n",
    "    continuous_features=continuous_feature_names,\n",
    "    category_dict=category_dict,\n",
    "    embed_dim=128,\n",
    "    drop_rate=0.1,\n",
    "    pre_encoded=False,\n",
    ")\n",
    "\n",
    "target_transform = TargetCreator(\n",
    "    cols = continuous_feature_names + categorical_feature_names,\n",
    "    emb=emb,\n",
    "    max_item_len=100,\n",
    "    start_token_discr=START_TOKEN_DISCR,\n",
    "    start_token_cont=START_TOKEN_CONT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the `SequenceDataModule` #TODO LINK TO DOCS automating a few tasks, such as creating the `DataLoaders` for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = SequenceDataModule(\n",
    "    dataset=dataset,\n",
    "    target_transform=target_transform,\n",
    "    test_size=0.2,\n",
    "    batch_points=1024,\n",
    "    device=device,\n",
    "    min_points=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will implement our model. This model contains the embedder, encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VariationalEventModel(\n",
    "    emb,\n",
    "    rnn_dim=128,\n",
    "    drop_rate=0.2,\n",
    "    bottleneck_dim=32,\n",
    "    lr=0.001,\n",
    "    target_cols=target_transform.cols,\n",
    "    vae_sample_z=True,\n",
    "    vae_sampling_scaler=1.0,\n",
    "    vae_KL_weight=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track the training progress let us launch a tensorboard. You can also launch the tensorboard in another tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model :) \n",
    "The `run_model` function sets up the training for you, as well, as the logging and checkpointing. The use of the `run_model` interface is optional, but will provide less experienced users a one-line interface to run their model.\n",
    "\n",
    "ATTENTION: The default number of epochs below is set to `2`. We recommend you set this to `100+` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_lifetimes.run_model(\n",
    "    datamodule,\n",
    "    net,\n",
    "    log_dir=LOG_DIR,\n",
    "    device_type=device_type,\n",
    "    num_epochs=2,\n",
    "    val_check_interval=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it. You can see how simple it is to fit a complicated neural network model on custom data using the `neural-lifetimes` library. If you have comments please check out \n",
    "\n",
    "https://github.com/transferwise/neural-lifetimes/blob/pandas-and-custom-dataset/examples/use_own_dataset.ipynb"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac40b0b571ca3fc8a5ec2a871eb30a1e4bbebf94e900383a118ae45fa73cc052"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
